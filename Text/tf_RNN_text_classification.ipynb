{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "print(train_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\nlabel:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "    print('test: ', example.numpy())\n",
    "    print('label: ', example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "texts:  [b'This is one of those movies that you keep thinking about when you wake up the next morning. It will give you that warm, fuzzy feeling and leave you with a smile on your face.<br /><br />Sure, we get fed the typical stereotype characters and stories, but it does do the trick: Entertain.<br /><br />Being from Sweden and living in the US for quite sometime, it is funny how we react. \"The deadbeat husband is going to kill him\", \"She (Gabriella) is going to die and then there will be a heartbreaking larger-than-life ending\". We know how these things work, everything comes together at the end. And it did. The characters were somewhat simple, they were so elaborate that you didn\\'t really think twice about it, nothing was really left for your own imagination. The closest would probably be Siv, she makes you ask yourself if she indeed was in love with Daniel, but that\\'s about it.<br /><br />But the movie is beautiful, set in rural Norrland, the music is absolutely amazing and the characters are lovable. Michael Nyqvist is truly genius, with his crazy unique look and Frida Hallberg is charming and approachable. Maybe a little too nice.<br /><br />But most of all this movie makes you feel, and that is the most important thing. You cry, you laugh, you hate and you identify. I don\\'t know about you guys, but that does not happen that often.'\n b'In Cold Blood was one of several 60s films that created a new vision of violence in the Hollywood film industry. Capote coined the phrase \"nonfiction novel\" to describe the book on which this film is based, and the spirit of that form was carried over into the film script, which he co-wrote. Despite the fact that we were well into the era of color film, Richard Brooks elected to present this film in black and white to underscore both the starkness of the landscape and the bleakness of the story. This is the first problem with the TV remake --color changes the tone of the story. In addition, the confinement of shooting a film for TV makes reduces the options of how the shots are framed and focused. As a result, we lose the dramatic clash which makes the second part of the original film (police interviews, trial, imprisonment, and execution) so claustrophobic. On the small screen, it\\'s just another version of Law and Order spin-offs. <br /><br />Hollywood\\'s search for scripts continuously takes it back to movies that were successful in another age. Usually, that\\'s a mistake, and this is no exception.<br /><br />All of the actors are competent. The script is OK. The directing doesn\\'t get in the way. It\\'s just that the movie doesn\\'t work as well as the original precision instrument. It doesn\\'t hook the viewer into the ambivalence toward Smith and Hickock that the original film provokes. At the end of the TV version, we are left with the feeling: \"Ho hum, who cares?\"<br /><br />See the original first, on as large a screen as you can, then watch the TV version simply to understand why the first one was such an important film in 1967.<br /><br />Wouldn\\'t hurt to also go on line and read a bit about Capote and the original book. It will help you to understand the extraordinary effort he put into the material, and also some of the controversy surrounding both the book and the movie.<br /><br />I actually only gave this a 4 because I save the bottom 3 rankings for true bombs--the kind that enrage you about having been sucked into spending an'\n b\"I went to this movie expecting a concise movie relating the effect the Son of Sam had on the society. I didn't expect Spike Lee to force-feed me more garbage on racial tension, mob-justice, or the inability of the common citizen to make a choice under pressure by peers. Lee has presented an extreme opinion.<br /><br />The entire movie could have been more effective if in a 90-min format with more focus, less tangential sub-plots.<br /><br />Don't even bother renting the video unless you passionately enjoy Spike Lee; in such a case, the theatre is worth it. This is not an escapist movie.\"]\n\nlabels:  [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "    print('texts: ', example.numpy()[:3])\n",
    "    print()\n",
    "    print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
       "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
       "      dtype='<U14')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "inverse_vocab = {index:key for index, key in enumerate(vocab)}\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 11   7  29 ...   0   0   0]\n [  8   1 535 ...   0   0   0]\n [ 10 418   6 ...   0   0   0]]\n['this', 'is', 'one', 'of', 'those', 'movies', 'that', 'you', 'keep', 'thinking', 'about', 'when', 'you', '[UNK]', 'up', 'the', 'next', '[UNK]', 'it', 'will', 'give', 'you', 'that', '[UNK]', '[UNK]', 'feeling', 'and', 'leave', 'you', 'with', 'a', '[UNK]', 'on', 'your', '[UNK]', 'br', 'sure', 'we', 'get', '[UNK]', 'the', 'typical', '[UNK]', 'characters', 'and', 'stories', 'but', 'it', 'does', 'do', 'the', '[UNK]', '[UNK]', 'br', 'being', 'from', '[UNK]', 'and', 'living', 'in', 'the', 'us', 'for', 'quite', '[UNK]', 'it', 'is', 'funny', 'how', 'we', '[UNK]', 'the', '[UNK]', 'husband', 'is', 'going', 'to', 'kill', 'him', 'she', '[UNK]', 'is', 'going', 'to', 'die', 'and', 'then', 'there', 'will', 'be', 'a', '[UNK]', '[UNK]', 'ending', 'we', 'know', 'how', 'these', 'things', 'work', 'everything', 'comes', 'together', 'at', 'the', 'end', 'and', 'it', 'did', 'the', 'characters', 'were', 'somewhat', 'simple', 'they', 'were', 'so', '[UNK]', 'that', 'you', 'didnt', 'really', 'think', '[UNK]', 'about', 'it', 'nothing', 'was', 'really', 'left', 'for', 'your', 'own', '[UNK]', 'the', '[UNK]', 'would', 'probably', 'be', '[UNK]', 'she', 'makes', 'you', 'ask', 'yourself', 'if', 'she', 'indeed', 'was', 'in', 'love', 'with', '[UNK]', 'but', 'thats', 'about', 'itbr', 'br', 'but', 'the', 'movie', 'is', 'beautiful', 'set', 'in', '[UNK]', '[UNK]', 'the', 'music', 'is', 'absolutely', 'amazing', 'and', 'the', 'characters', 'are', '[UNK]', 'michael', '[UNK]', 'is', 'truly', '[UNK]', 'with', 'his', 'crazy', 'unique', 'look', 'and', '[UNK]', '[UNK]', 'is', '[UNK]', 'and', '[UNK]', 'maybe', 'a', 'little', 'too', '[UNK]', 'br', 'but', 'most', 'of', 'all', 'this', 'movie', 'makes', 'you', 'feel', 'and', 'that', 'is', 'the', 'most', 'important', 'thing', 'you', '[UNK]', 'you', 'laugh', 'you', 'hate', 'and', 'you', '[UNK]', 'i', 'dont', 'know', 'about', 'you', 'guys', 'but', 'that', 'does', 'not', 'happen', 'that', 'often', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n['in', '[UNK]', 'blood', 'was', 'one', 'of', 'several', '[UNK]', 'films', 'that', '[UNK]', 'a', 'new', '[UNK]', 'of', 'violence', 'in', 'the', 'hollywood', 'film', '[UNK]', '[UNK]', '[UNK]', 'the', '[UNK]', '[UNK]', 'novel', 'to', '[UNK]', 'the', 'book', 'on', 'which', 'this', 'film', 'is', 'based', 'and', 'the', '[UNK]', 'of', 'that', 'form', 'was', '[UNK]', 'over', 'into', 'the', 'film', 'script', 'which', 'he', '[UNK]', 'despite', 'the', 'fact', 'that', 'we', 'were', 'well', 'into', 'the', '[UNK]', 'of', '[UNK]', 'film', 'richard', '[UNK]', '[UNK]', 'to', 'present', 'this', 'film', 'in', 'black', 'and', 'white', 'to', '[UNK]', 'both', 'the', '[UNK]', 'of', 'the', '[UNK]', 'and', 'the', '[UNK]', 'of', 'the', 'story', 'this', 'is', 'the', 'first', 'problem', 'with', 'the', 'tv', 'remake', '[UNK]', '[UNK]', 'the', '[UNK]', 'of', 'the', 'story', 'in', '[UNK]', 'the', '[UNK]', 'of', '[UNK]', 'a', 'film', 'for', 'tv', 'makes', '[UNK]', 'the', '[UNK]', 'of', 'how', 'the', 'shots', 'are', '[UNK]', 'and', '[UNK]', 'as', 'a', 'result', 'we', '[UNK]', 'the', 'dramatic', '[UNK]', 'which', 'makes', 'the', 'second', 'part', 'of', 'the', 'original', 'film', 'police', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', 'so', '[UNK]', 'on', 'the', 'small', 'screen', 'its', 'just', 'another', 'version', 'of', '[UNK]', 'and', 'order', '[UNK]', 'br', 'br', '[UNK]', '[UNK]', 'for', '[UNK]', '[UNK]', 'takes', 'it', 'back', 'to', 'movies', 'that', 'were', '[UNK]', 'in', 'another', 'age', 'usually', 'thats', 'a', '[UNK]', 'and', 'this', 'is', 'no', '[UNK]', 'br', 'all', 'of', 'the', 'actors', 'are', '[UNK]', 'the', 'script', 'is', 'ok', 'the', 'directing', 'doesnt', 'get', 'in', 'the', 'way', 'its', 'just', 'that', 'the', 'movie', 'doesnt', 'work', 'as', 'well', 'as', 'the', 'original', '[UNK]', '[UNK]', 'it', 'doesnt', '[UNK]', 'the', 'viewer', 'into', 'the', '[UNK]', '[UNK]', '[UNK]', 'and', '[UNK]', 'that', 'the', 'original', 'film', '[UNK]', 'at', 'the', 'end', 'of', 'the', 'tv', 'version', 'we', 'are', 'left', 'with', 'the', 'feeling', '[UNK]', '[UNK]', 'who', '[UNK]', 'br', 'see', 'the', 'original', 'first', 'on', 'as', '[UNK]', 'a', 'screen', 'as', 'you', 'can', 'then', 'watch', 'the', 'tv', 'version', 'simply', 'to', 'understand', 'why', 'the', 'first', 'one', 'was', 'such', 'an', 'important', 'film', 'in', '[UNK]', 'br', 'wouldnt', '[UNK]', 'to', 'also', 'go', 'on', 'line', 'and', 'read', 'a', 'bit', 'about', '[UNK]', 'and', 'the', 'original', 'book', 'it', 'will', 'help', 'you', 'to', 'understand', 'the', '[UNK]', 'effort', 'he', 'put', 'into', 'the', 'material', 'and', 'also', 'some', 'of', 'the', '[UNK]', '[UNK]', 'both', 'the', 'book', 'and', 'the', 'moviebr', 'br', 'i', 'actually', 'only', 'gave', 'this', 'a', '4', 'because', 'i', 'save', 'the', '[UNK]', '3', '[UNK]', 'for', 'true', '[UNK]', 'kind', 'that', '[UNK]', 'you', 'about', 'having', 'been', '[UNK]', 'into', '[UNK]', 'an', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n['i', 'went', 'to', 'this', 'movie', 'expecting', 'a', '[UNK]', 'movie', '[UNK]', 'the', 'effect', 'the', 'son', 'of', '[UNK]', 'had', 'on', 'the', 'society', 'i', 'didnt', 'expect', '[UNK]', 'lee', 'to', '[UNK]', 'me', 'more', '[UNK]', 'on', '[UNK]', '[UNK]', '[UNK]', 'or', 'the', '[UNK]', 'of', 'the', '[UNK]', '[UNK]', 'to', 'make', 'a', '[UNK]', 'under', '[UNK]', 'by', '[UNK]', 'lee', 'has', '[UNK]', 'an', '[UNK]', '[UNK]', 'br', 'the', 'entire', 'movie', 'could', 'have', 'been', 'more', '[UNK]', 'if', 'in', 'a', '[UNK]', '[UNK]', 'with', 'more', '[UNK]', 'less', '[UNK]', '[UNK]', 'br', 'dont', 'even', '[UNK]', '[UNK]', 'the', 'video', 'unless', 'you', '[UNK]', 'enjoy', '[UNK]', 'lee', 'in', 'such', 'a', 'case', 'the', '[UNK]', 'is', 'worth', 'it', 'this', 'is', 'not', 'an', '[UNK]', 'movie', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "print(encoded_example)\n",
    "for seq in encoded_example:\n",
    "    print([inverse_vocab[index] for index in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original:  b'This is one of those movies that you keep thinking about when you wake up the next morning. It will give you that warm, fuzzy feeling and leave you with a smile on your face.<br /><br />Sure, we get fed the typical stereotype characters and stories, but it does do the trick: Entertain.<br /><br />Being from Sweden and living in the US for quite sometime, it is funny how we react. \"The deadbeat husband is going to kill him\", \"She (Gabriella) is going to die and then there will be a heartbreaking larger-than-life ending\". We know how these things work, everything comes together at the end. And it did. The characters were somewhat simple, they were so elaborate that you didn\\'t really think twice about it, nothing was really left for your own imagination. The closest would probably be Siv, she makes you ask yourself if she indeed was in love with Daniel, but that\\'s about it.<br /><br />But the movie is beautiful, set in rural Norrland, the music is absolutely amazing and the characters are lovable. Michael Nyqvist is truly genius, with his crazy unique look and Frida Hallberg is charming and approachable. Maybe a little too nice.<br /><br />But most of all this movie makes you feel, and that is the most important thing. You cry, you laugh, you hate and you identify. I don\\'t know about you guys, but that does not happen that often.'\nRound-trip:  this is one of those movies that you keep thinking about when you [UNK] up the next [UNK] it will give you that [UNK] [UNK] feeling and leave you with a [UNK] on your [UNK] br sure we get [UNK] the typical [UNK] characters and stories but it does do the [UNK] [UNK] br being from [UNK] and living in the us for quite [UNK] it is funny how we [UNK] the [UNK] husband is going to kill him she [UNK] is going to die and then there will be a [UNK] [UNK] ending we know how these things work everything comes together at the end and it did the characters were somewhat simple they were so [UNK] that you didnt really think [UNK] about it nothing was really left for your own [UNK] the [UNK] would probably be [UNK] she makes you ask yourself if she indeed was in love with [UNK] but thats about itbr br but the movie is beautiful set in [UNK] [UNK] the music is absolutely amazing and the characters are [UNK] michael [UNK] is truly [UNK] with his crazy unique look and [UNK] [UNK] is [UNK] and [UNK] maybe a little too [UNK] br but most of all this movie makes you feel and that is the most important thing you [UNK] you laugh you hate and you [UNK] i dont know about you guys but that does not happen that often                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\nOriginal:  b'In Cold Blood was one of several 60s films that created a new vision of violence in the Hollywood film industry. Capote coined the phrase \"nonfiction novel\" to describe the book on which this film is based, and the spirit of that form was carried over into the film script, which he co-wrote. Despite the fact that we were well into the era of color film, Richard Brooks elected to present this film in black and white to underscore both the starkness of the landscape and the bleakness of the story. This is the first problem with the TV remake --color changes the tone of the story. In addition, the confinement of shooting a film for TV makes reduces the options of how the shots are framed and focused. As a result, we lose the dramatic clash which makes the second part of the original film (police interviews, trial, imprisonment, and execution) so claustrophobic. On the small screen, it\\'s just another version of Law and Order spin-offs. <br /><br />Hollywood\\'s search for scripts continuously takes it back to movies that were successful in another age. Usually, that\\'s a mistake, and this is no exception.<br /><br />All of the actors are competent. The script is OK. The directing doesn\\'t get in the way. It\\'s just that the movie doesn\\'t work as well as the original precision instrument. It doesn\\'t hook the viewer into the ambivalence toward Smith and Hickock that the original film provokes. At the end of the TV version, we are left with the feeling: \"Ho hum, who cares?\"<br /><br />See the original first, on as large a screen as you can, then watch the TV version simply to understand why the first one was such an important film in 1967.<br /><br />Wouldn\\'t hurt to also go on line and read a bit about Capote and the original book. It will help you to understand the extraordinary effort he put into the material, and also some of the controversy surrounding both the book and the movie.<br /><br />I actually only gave this a 4 because I save the bottom 3 rankings for true bombs--the kind that enrage you about having been sucked into spending an'\nRound-trip:  in [UNK] blood was one of several [UNK] films that [UNK] a new [UNK] of violence in the hollywood film [UNK] [UNK] [UNK] the [UNK] [UNK] novel to [UNK] the book on which this film is based and the [UNK] of that form was [UNK] over into the film script which he [UNK] despite the fact that we were well into the [UNK] of [UNK] film richard [UNK] [UNK] to present this film in black and white to [UNK] both the [UNK] of the [UNK] and the [UNK] of the story this is the first problem with the tv remake [UNK] [UNK] the [UNK] of the story in [UNK] the [UNK] of [UNK] a film for tv makes [UNK] the [UNK] of how the shots are [UNK] and [UNK] as a result we [UNK] the dramatic [UNK] which makes the second part of the original film police [UNK] [UNK] [UNK] and [UNK] so [UNK] on the small screen its just another version of [UNK] and order [UNK] br br [UNK] [UNK] for [UNK] [UNK] takes it back to movies that were [UNK] in another age usually thats a [UNK] and this is no [UNK] br all of the actors are [UNK] the script is ok the directing doesnt get in the way its just that the movie doesnt work as well as the original [UNK] [UNK] it doesnt [UNK] the viewer into the [UNK] [UNK] [UNK] and [UNK] that the original film [UNK] at the end of the tv version we are left with the feeling [UNK] [UNK] who [UNK] br see the original first on as [UNK] a screen as you can then watch the tv version simply to understand why the first one was such an important film in [UNK] br wouldnt [UNK] to also go on line and read a bit about [UNK] and the original book it will help you to understand the [UNK] effort he put into the material and also some of the [UNK] [UNK] both the book and the moviebr br i actually only gave this a 4 because i save the [UNK] 3 [UNK] for true [UNK] kind that [UNK] you about having been [UNK] into [UNK] an                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n\nOriginal:  b\"I went to this movie expecting a concise movie relating the effect the Son of Sam had on the society. I didn't expect Spike Lee to force-feed me more garbage on racial tension, mob-justice, or the inability of the common citizen to make a choice under pressure by peers. Lee has presented an extreme opinion.<br /><br />The entire movie could have been more effective if in a 90-min format with more focus, less tangential sub-plots.<br /><br />Don't even bother renting the video unless you passionately enjoy Spike Lee; in such a case, the theatre is worth it. This is not an escapist movie.\"\nRound-trip:  i went to this movie expecting a [UNK] movie [UNK] the effect the son of [UNK] had on the society i didnt expect [UNK] lee to [UNK] me more [UNK] on [UNK] [UNK] [UNK] or the [UNK] of the [UNK] [UNK] to make a [UNK] under [UNK] by [UNK] lee has [UNK] an [UNK] [UNK] br the entire movie could have been more [UNK] if in a [UNK] [UNK] with more [UNK] less [UNK] [UNK] br dont even [UNK] [UNK] the video unless you [UNK] enjoy [UNK] lee in such a case the [UNK] is worth it this is not an [UNK] movie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    print(\"Original: \", example[n].numpy())\n",
    "    print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.00440053]\n"
     ]
    }
   ],
   "source": [
    "sample_text = ('The movie was cool. The animation and the graphics were out of this world. I would recommend this movie.')\n",
    "\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.00440053]\n"
     ]
    }
   ],
   "source": [
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sample_text, padding]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "197/391 [==============>...............] - ETA: 2:03 - loss: 0.6928 - accuracy: 0.5044"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a12d039d9d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset, validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"Test Accuracy\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ('I am fairly ambivalent to the movie. It was neither good, nor bad. It just was.')\n",
    "\n",
    "predictions = model.predict(np.array([sample_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}